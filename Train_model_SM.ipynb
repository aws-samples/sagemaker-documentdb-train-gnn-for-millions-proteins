{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ddb1017",
   "metadata": {},
   "source": [
    "# Training Amazon SageMaker models by using the Deep Graph Library with PyTorch backend\n",
    "\n",
    "https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/dgl_gcn/pytorch_gcn.ipynb\n",
    "\n",
    "## Setup\n",
    "\n",
    "Define a few variables that are needed later in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20ec3d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "# Setup session\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket here.\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "# # Location to put your custom code.\n",
    "# custom_code_upload_location = \"customcode\"\n",
    "\n",
    "# IAM execution role that gives Amazon SageMaker access to resources in your AWS account.\n",
    "# You can use the Amazon SageMaker Python SDK to get the role from the notebook environment.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56295222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::811425317877:role/JanssenMLSLSageMakerNotebookRole'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9506bb04",
   "metadata": {},
   "source": [
    "## The training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b8716a1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import argparse\n",
      "import json\n",
      "import os\n",
      "import math\n",
      "import random\n",
      "from datetime import datetime\n",
      "\n",
      "from pymongo import MongoClient\n",
      "import dgl\n",
      "from dgl.nn import GraphConv\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.utils.data as data\n",
      "import torch.nn.functional as F\n",
      "from sklearn.metrics import roc_auc_score\n",
      "\n",
      "# amino acid to index mapping\n",
      "# from Bio.PDB.Polypeptide import d1_to_index\n",
      "d1_to_index = {\n",
      "    \"A\": 0,\n",
      "    \"C\": 1,\n",
      "    \"D\": 2,\n",
      "    \"E\": 3,\n",
      "    \"F\": 4,\n",
      "    \"G\": 5,\n",
      "    \"H\": 6,\n",
      "    \"I\": 7,\n",
      "    \"K\": 8,\n",
      "    \"L\": 9,\n",
      "    \"M\": 10,\n",
      "    \"N\": 11,\n",
      "    \"P\": 12,\n",
      "    \"Q\": 13,\n",
      "    \"R\": 14,\n",
      "    \"S\": 15,\n",
      "    \"T\": 16,\n",
      "    \"V\": 17,\n",
      "    \"W\": 18,\n",
      "    \"Y\": 19,\n",
      "    \"X\": 20,\n",
      "}\n",
      "\n",
      "\n",
      "def setup(args, seed=0):\n",
      "    args[\"device\"] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "\n",
      "    # Set random seed\n",
      "    random.seed(seed)\n",
      "    np.random.seed(seed)\n",
      "    torch.manual_seed(seed)\n",
      "    if torch.cuda.is_available():\n",
      "        torch.cuda.manual_seed(seed)\n",
      "    return args\n",
      "\n",
      "\n",
      "def collate_protein_graphs(samples):\n",
      "    \"\"\"Batching a list of datapoints for dataloader.\"\"\"\n",
      "    graphs, targets = map(list, zip(*samples))\n",
      "    bg = dgl.batch(graphs)\n",
      "    return bg, torch.tensor(targets).unsqueeze(1).to(torch.float32)\n",
      "\n",
      "\n",
      "def _convert_to_graph(protein):\n",
      "    \"\"\"\n",
      "    Convert a protein (dict) to a dgl graph\n",
      "    \"\"\"\n",
      "    coords = torch.tensor(protein[\"coords\"])\n",
      "    X_ca = coords[:, 1]\n",
      "    # construct knn graph from C-alpha coordinates\n",
      "    g = dgl.knn_graph(X_ca, k=2)\n",
      "    seq = protein[\"seq\"]\n",
      "    node_features = torch.tensor([d1_to_index[residue] for residue in seq])\n",
      "    node_features = F.one_hot(node_features, num_classes=len(d1_to_index)).to(\n",
      "        dtype=torch.float\n",
      "    )\n",
      "\n",
      "    # add node features\n",
      "    g.ndata[\"h\"] = node_features\n",
      "    return g\n",
      "\n",
      "\n",
      "class ProteinDataset(data.IterableDataset):\n",
      "    \"\"\"\n",
      "    An iterable-style dataset for proteins in DocumentDB\n",
      "    Args:\n",
      "        - pipeline: an aggregation pipeline to retrieve data from DocumentDB\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, pipeline, db_uri=\"\", db_name=\"\", collection_name=\"\"):\n",
      "\n",
      "        self.db_uri = db_uri\n",
      "        self.db_name = db_name\n",
      "        self.collection_name = collection_name\n",
      "\n",
      "        with MongoClient(self.db_uri) as client:\n",
      "            collection = client[self.db_name][self.collection_name]\n",
      "            # pre-fetch the metadata as docs from DocumentDB\n",
      "            self.docs = [doc for doc in collection.aggregate(pipeline)]\n",
      "        # mapping document '_id' to label\n",
      "        self.labels = {doc[\"_id\"]: doc[\"y\"] for doc in self.docs}\n",
      "\n",
      "    def __iter__(self):\n",
      "        worker_info = torch.utils.data.get_worker_info()\n",
      "        if (\n",
      "            worker_info is None\n",
      "        ):  # single-process data loading, return the full iterator\n",
      "            protein_ids = [doc[\"_id\"] for doc in self.docs]\n",
      "\n",
      "        else:  # in a worker process\n",
      "            # split workload\n",
      "            start = 0\n",
      "            end = len(self.docs)\n",
      "            per_worker = int(\n",
      "                math.ceil((end - start) / float(worker_info.num_workers))\n",
      "            )\n",
      "            worker_id = worker_info.id\n",
      "            iter_start = start + worker_id * per_worker\n",
      "            iter_end = min(iter_start + per_worker, end)\n",
      "\n",
      "            protein_ids = [\n",
      "                doc[\"_id\"] for doc in self.docs[iter_start:iter_end]\n",
      "            ]\n",
      "\n",
      "        # retrieve a list of proteins by _id from DocDB\n",
      "        with MongoClient(self.db_uri) as client:\n",
      "            collection = client[self.db_name][self.collection_name]\n",
      "            cur = collection.find(\n",
      "                {\"_id\": {\"$in\": protein_ids}},\n",
      "                projection={\"coords\": True, \"seq\": True},\n",
      "            )\n",
      "            return (\n",
      "                (_convert_to_graph(protein), self.labels[protein[\"_id\"]])\n",
      "                for protein in cur\n",
      "            )\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.docs)\n",
      "\n",
      "\n",
      "class BufferedShuffleDataset(data.IterableDataset):\n",
      "    \"\"\"Dataset shuffled from the original dataset.\n",
      "    This class is useful to shuffle an existing instance of an IterableDataset.\n",
      "    \"\"\"\n",
      "\n",
      "    dataset: data.IterableDataset\n",
      "    buffer_size: int\n",
      "\n",
      "    def __init__(\n",
      "        self, dataset: data.IterableDataset, buffer_size: int\n",
      "    ) -> None:\n",
      "        super(BufferedShuffleDataset, self).__init__()\n",
      "        assert buffer_size > 0, \"buffer_size should be larger than 0\"\n",
      "        self.dataset = dataset\n",
      "        self.buffer_size = buffer_size\n",
      "\n",
      "    def __iter__(self):\n",
      "        buf = []\n",
      "        for x in self.dataset:\n",
      "            if len(buf) == self.buffer_size:\n",
      "                idx = random.randint(0, self.buffer_size - 1)\n",
      "                yield buf[idx]\n",
      "                buf[idx] = x\n",
      "            else:\n",
      "                buf.append(x)\n",
      "        random.shuffle(buf)\n",
      "        while buf:\n",
      "            yield buf.pop()\n",
      "\n",
      "\n",
      "class EarlyStopper(object):\n",
      "    def __init__(self, patience, filename=None):\n",
      "        if filename is None:\n",
      "            # Name checkpoint based on time\n",
      "            dt = datetime.now()\n",
      "            filename = \"early_stop_{}_{:02d}-{:02d}-{:02d}.pth\".format(\n",
      "                dt.date(), dt.hour, dt.minute, dt.second\n",
      "            )\n",
      "            filename = os.path.join(\"/opt/ml/model\", filename)\n",
      "\n",
      "        self.patience = patience\n",
      "        self.counter = 0\n",
      "        self.filename = filename\n",
      "        self.best_score = None\n",
      "        self.early_stop = False\n",
      "\n",
      "    def save_checkpoint(self, model):\n",
      "        \"\"\"Saves model when the metric on the validation set gets improved.\"\"\"\n",
      "        torch.save({\"model_state_dict\": model.state_dict()}, self.filename)\n",
      "\n",
      "    def load_checkpoint(self, model):\n",
      "        \"\"\"Load model saved with early stopping.\"\"\"\n",
      "        model.load_state_dict(torch.load(self.filename)[\"model_state_dict\"])\n",
      "\n",
      "    def step(self, score, model):\n",
      "        if (self.best_score is None) or (score > self.best_score):\n",
      "            self.best_score = score\n",
      "            self.save_checkpoint(model)\n",
      "            self.counter = 0\n",
      "        else:\n",
      "            self.counter += 1\n",
      "            print(\n",
      "                \"EarlyStopping counter: {:d} out of {:d}\".format(\n",
      "                    self.counter, self.patience\n",
      "                )\n",
      "            )\n",
      "            if self.counter >= self.patience:\n",
      "                self.early_stop = True\n",
      "        return self.early_stop\n",
      "\n",
      "\n",
      "class Meter(object):\n",
      "    \"\"\"Track and summarize model performance on a dataset for\n",
      "    (multi-label) binary classification.\"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        self.y_pred = []\n",
      "        self.y_true = []\n",
      "\n",
      "    def update(self, y_pred, y_true):\n",
      "        \"\"\"Update for the result of an iteration\n",
      "        Parameters\n",
      "        ----------\n",
      "        y_pred : float32 tensor\n",
      "            Predicted molecule labels with shape (B, T),\n",
      "            B for batch size and T for the number of tasks\n",
      "        y_true : float32 tensor\n",
      "            Ground truth molecule labels with shape (B, T)\n",
      "        \"\"\"\n",
      "        self.y_pred.append(y_pred.detach().cpu())\n",
      "        self.y_true.append(y_true.detach().cpu())\n",
      "\n",
      "    def roc_auc_score(self):\n",
      "        \"\"\"Compute roc-auc score for each task.\n",
      "        Returns\n",
      "        -------\n",
      "        list of float\n",
      "            roc-auc score for all tasks\n",
      "        \"\"\"\n",
      "        y_pred = torch.cat(self.y_pred, dim=0)\n",
      "        y_true = torch.cat(self.y_true, dim=0)\n",
      "        # This assumes binary case only\n",
      "        y_pred = torch.sigmoid(y_pred)\n",
      "        n_tasks = y_true.shape[1]\n",
      "        scores = []\n",
      "        for task in range(n_tasks):\n",
      "            task_y_true = y_true[:, task].numpy()\n",
      "            task_y_pred = y_pred[:, task].numpy()\n",
      "            scores.append(roc_auc_score(task_y_true, task_y_pred))\n",
      "        return scores\n",
      "\n",
      "\n",
      "class GCN(nn.Module):\n",
      "    def __init__(self, in_feats, h_feats, num_classes):\n",
      "        super(GCN, self).__init__()\n",
      "        self.conv1 = GraphConv(in_feats, h_feats)\n",
      "        self.conv2 = GraphConv(h_feats, num_classes)\n",
      "\n",
      "    def forward(self, g, in_feat):\n",
      "        h = self.conv1(g, in_feat)\n",
      "        h = F.relu(h)\n",
      "        h = self.conv2(g, h)\n",
      "        g.ndata[\"h\"] = h\n",
      "        return dgl.mean_nodes(g, \"h\")\n",
      "\n",
      "\n",
      "def run_a_train_epoch(args, epoch, model, data_loader, optimizer):\n",
      "    model.train()\n",
      "    train_meter = Meter()\n",
      "    for batch_id, batch_data in enumerate(data_loader):\n",
      "        bg, labels = batch_data\n",
      "        bg = bg.to(args[\"device\"])\n",
      "        labels = labels.to(args[\"device\"])\n",
      "        logits = model(bg, bg.ndata[\"h\"])\n",
      "        # Mask non-existing labels\n",
      "        loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        print(\n",
      "            \"epoch {:d}/{:d}, batch {:d}, loss {:.4f}\".format(\n",
      "                epoch + 1,\n",
      "                args[\"n_epochs\"],\n",
      "                batch_id + 1,\n",
      "                loss.item(),\n",
      "            )\n",
      "        )\n",
      "        train_meter.update(logits, labels)\n",
      "    train_score = np.mean(train_meter.roc_auc_score())\n",
      "    print(\n",
      "        \"epoch {:d}/{:d}, training roc-auc {:.4f}\".format(\n",
      "            epoch + 1, args[\"n_epochs\"], train_score\n",
      "        )\n",
      "    )\n",
      "\n",
      "\n",
      "def run_an_eval_epoch(args, model, data_loader):\n",
      "    model.eval()\n",
      "    eval_meter = Meter()\n",
      "    with torch.no_grad():\n",
      "        for batch_data in data_loader:\n",
      "            bg, labels = batch_data\n",
      "            bg = bg.to(args[\"device\"])\n",
      "            labels = labels.to(args[\"device\"])\n",
      "            logits = model(bg, bg.ndata[\"h\"])\n",
      "            eval_meter.update(logits, labels)\n",
      "    return np.mean(eval_meter.roc_auc_score())\n",
      "\n",
      "\n",
      "def load_sagemaker_config(args):\n",
      "    file_path = \"/opt/ml/input/config/hyperparameters.json\"\n",
      "    if os.path.isfile(file_path):\n",
      "        with open(file_path, \"r\") as f:\n",
      "            new_args = json.load(f)\n",
      "            for k, v in new_args.items():\n",
      "                if k not in args:\n",
      "                    continue\n",
      "                if isinstance(args[k], int):\n",
      "                    v = int(v)\n",
      "                if isinstance(args[k], float):\n",
      "                    v = float(v)\n",
      "                args[k] = v\n",
      "    return args\n",
      "\n",
      "\n",
      "def match_by_split(split):\n",
      "    return {\"$and\": [{\"is_AF\": {\"$exists\": True}}, {\"split\": split}]}\n",
      "\n",
      "\n",
      "def main(args):\n",
      "    args = setup(args)\n",
      "    uri = \"mongodb://{}:{}@{}:27017/?tls=true&tlsCAFile=rds-combined-ca-bundle.pem&replicaSet=rs0&readPreference=secondaryPreferred&retryWrites=false\".format(\n",
      "        args[\"db_username\"], args[\"db_password\"], args[\"db_host\"]\n",
      "    )\n",
      "\n",
      "    project = {\"y\": \"$is_AF\"}\n",
      "\n",
      "    datasets = [\n",
      "        ProteinDataset(\n",
      "            [\n",
      "                {\"$match\": match_by_split(split)},\n",
      "                {\"$project\": project},\n",
      "            ],\n",
      "            db_uri=uri,\n",
      "            db_name=\"proteins\",\n",
      "            collection_name=\"proteins\",\n",
      "        )\n",
      "        for split in (\"train\", \"valid\", \"test\")\n",
      "    ]\n",
      "    train_loader = data.DataLoader(\n",
      "        BufferedShuffleDataset(datasets[0], buffer_size=256),\n",
      "        batch_size=args.batch_size,\n",
      "        collate_fn=collate_protein_graphs,\n",
      "        num_workers=8,\n",
      "    )\n",
      "\n",
      "    valid_loader = data.DataLoader(\n",
      "        datasets[1],\n",
      "        batch_size=args.batch_size,\n",
      "        collate_fn=collate_protein_graphs,\n",
      "    )\n",
      "    test_loader = data.DataLoader(\n",
      "        datasets[2],\n",
      "        batch_size=args.batch_size,\n",
      "        collate_fn=collate_protein_graphs,\n",
      "    )\n",
      "\n",
      "    # Create the model with given dimensions\n",
      "    dim_nfeats = len(d1_to_index)\n",
      "    n_classes = 1\n",
      "    model = GCN(dim_nfeats, 16, n_classes).to(args[\"device\"])\n",
      "\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
      "    stopper = EarlyStopper(args[\"patience\"])\n",
      "\n",
      "    for epoch in range(args[\"n_epochs\"]):\n",
      "        # Train\n",
      "        run_a_train_epoch(args, epoch, model, train_loader, optimizer)\n",
      "\n",
      "        # Validation and early stop\n",
      "        val_score = run_an_eval_epoch(args, model, valid_loader)\n",
      "        early_stop = stopper.step(val_score, model)\n",
      "        print(\n",
      "            \"epoch {:d}/{:d}, validation roc-auc {:.4f}, best validation roc-auc {:.4f}\".format(\n",
      "                epoch + 1, args[\"n_epochs\"], val_score, stopper.best_score\n",
      "            )\n",
      "        )\n",
      "        if early_stop:\n",
      "            break\n",
      "\n",
      "    stopper.load_checkpoint(model)\n",
      "    test_score = run_an_eval_epoch(args, model, test_loader)\n",
      "    print(\"Best validation score {:.4f}\".format(stopper.best_score))\n",
      "    print(\"Test score {:.4f}\".format(test_score))\n",
      "\n",
      "\n",
      "def parse_args():\n",
      "    parser = argparse.ArgumentParser(description=\"GCN for Tox21\")\n",
      "    parser.add_argument(\n",
      "        \"--batch-size\",\n",
      "        type=int,\n",
      "        default=128,\n",
      "        help=\"Number of graphs (molecules) per batch\",\n",
      "    )\n",
      "    parser.add_argument(\"--lr\", type=float, default=1e-3, help=\"Learning rate\")\n",
      "    parser.add_argument(\n",
      "        \"--n-epochs\",\n",
      "        type=int,\n",
      "        default=10,\n",
      "        help=\"Maximum number of training epochs\",\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--db-host\",\n",
      "        type=str,\n",
      "        help=\"Host of DocumentDB\",\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--db-username\",\n",
      "        type=str,\n",
      "        help=\"Username of DocumentDB\",\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--db-password\",\n",
      "        type=str,\n",
      "        help=\"Password of DocumentDB\",\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--patience\",\n",
      "        type=int,\n",
      "        default=5,\n",
      "        help=\"Number of epochs to wait before early stop\",\n",
      "    )\n",
      "    return parser.parse_args().__dict__\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    args = parse_args()\n",
      "    args = load_sagemaker_config(args)\n",
      "    main(args)\n"
     ]
    }
   ],
   "source": [
    "!cat src/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a833488",
   "metadata": {},
   "source": [
    "## SageMaker's estimator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dae45661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "secrets = json.load(open('DocumentDB_secrets.json', 'r')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c5ffb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "CODE_PATH = \"main.py\"\n",
    "account = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = sess.boto_session.region_name\n",
    "\n",
    "params = {\n",
    "    'patience': 5, \n",
    "    'n-epochs': 10,\n",
    "    'db-host': secrets['host'],\n",
    "    'db-username': secrets['db_username'], \n",
    "    'db-password': secrets['db_password'], \n",
    "    \n",
    "}\n",
    "task_tags = [{\"Key\": \"ML Task\", \"Value\": \"DGL\"}]\n",
    "estimator = PyTorch(\n",
    "    entry_point=CODE_PATH,\n",
    "    source_dir='src',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "#     instance_type=\"ml.p3.2xlarge\",\n",
    "    instance_type='ml.c4.2xlarge',\n",
    "#     framework_version=\"1.3.1\",\n",
    "    framework_version=\"1.7.1\",\n",
    "    py_version=\"py3\",\n",
    "    debugger_hook_config=False,\n",
    "    tags=task_tags,\n",
    "    hyperparameters=params,\n",
    "    sagemaker_session=sess,\n",
    "    subnets=['subnet-e008bdbf'],\n",
    "    security_group_ids=['sg-069bf37128d412109', 'sg-026342aa24fe27af0'],\n",
    "#     vpc_config_override={\n",
    "#         'Subnets': ['subnet-e008bdbf'],\n",
    "#         'SecurityGroupIds': ['sg-069bf37128d412109', 'sg-026342aa24fe27af0']\n",
    "#     }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1920a72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.session.Session at 0x7f89a83a9748>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09ea1100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Subnets': ['subnet-e008bdbf'],\n",
       " 'SecurityGroupIds': ['sg-069bf37128d412109', 'sg-026342aa24fe27af0']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.get_vpc_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19e7efc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sagemaker.pytorch.estimator.PyTorch"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c92a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18a723e2",
   "metadata": {},
   "source": [
    "## Running the Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891050e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-25 17:52:44 Starting - Starting the training job...\n",
      "2021-08-25 17:53:02 Starting - Launching requested ML instancesProfilerReport-1629913964: InProgress\n",
      ".........\n",
      "2021-08-25 17:54:43 Starting - Preparing the instances for training......\n",
      "2021-08-25 17:55:43 Downloading - Downloading input data...\n",
      "2021-08-25 17:56:14 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-08-25 17:56:15,724 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-08-25 17:56:15,726 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-08-25 17:56:15,737 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-08-25 17:56:16,365 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca107bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
