{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b82977a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95f8b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets = json.load(open('DocumentDB_secrets.json', 'r')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee5fc3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TLS enabled\n",
    "uri = 'mongodb://{}:{}@{}:27017/?tls=true&tlsCAFile=rds-combined-ca-bundle.pem&replicaSet=rs0&readPreference=secondaryPreferred&retryWrites=false'\\\n",
    "    .format(secrets['db_username'], secrets['db_password'], secrets['host'])\n",
    "\n",
    "client = MongoClient(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9daf14da",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client['proteins']\n",
    "collection = db['proteins']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aa3231",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "404aa313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB.Polypeptide import d1_to_index, three_to_one\n",
    "\n",
    "# d1_to_index['X'] = len(d1_to_index) # encode uncommon residue as 20\n",
    "d1_to_index['X'] = 20\n",
    "\n",
    "def _convert_to_graph(protein):\n",
    "    '''\n",
    "    Convert a protein (dict) to a dgl graph\n",
    "    '''\n",
    "    coords = torch.tensor(protein['coords'])\n",
    "    X_ca = coords[:, 1]\n",
    "    # construct knn graph from C-alpha coordinates\n",
    "    g = dgl.knn_graph(X_ca, k=2)        \n",
    "    seq = protein['seq']\n",
    "    node_features = torch.tensor([d1_to_index[residue] for residue in seq])\n",
    "    node_features = F.one_hot(node_features, num_classes=len(d1_to_index)).to(dtype=torch.float)\n",
    "\n",
    "    # add node features\n",
    "    g.ndata[\"h\"] = node_features\n",
    "    return g    \n",
    "\n",
    "\n",
    "class ProteinDataset(data.IterableDataset):\n",
    "    \"\"\"\n",
    "    An iterable-style dataset for proteins in DocumentDB\n",
    "    Args:\n",
    "        - pipeline: an aggregation pipeline to retrieve data from DocumentDB\n",
    "    \"\"\"\n",
    "    def __init__(self, pipeline, db_uri='', db_name='', collection_name=''):\n",
    "        \n",
    "        self.db_uri = db_uri\n",
    "        self.db_name = db_name\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        with MongoClient(self.db_uri) as client:\n",
    "            collection = client[self.db_name][self.collection_name]\n",
    "            # pre-fetch the metadata as docs from DocumentDB\n",
    "            self.docs = [doc for doc in collection.aggregate(pipeline)]\n",
    "        # mapping document '_id' to label\n",
    "        self.labels = {doc['_id']: doc[\"y\"] for doc in self.docs}\n",
    "    \n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:  # single-process data loading, return the full iterator\n",
    "            protein_ids = [doc['_id'] for doc in self.docs]\n",
    "            \n",
    "        else:  # in a worker process\n",
    "            # split workload\n",
    "            start = 0\n",
    "            end = len(self.docs)\n",
    "            per_worker = int(math.ceil((end - start) / float(worker_info.num_workers)))\n",
    "            worker_id = worker_info.id\n",
    "            iter_start = start + worker_id * per_worker\n",
    "            iter_end = min(iter_start + per_worker, end)\n",
    "            \n",
    "            protein_ids = [doc['_id'] for doc in self.docs[iter_start:iter_end]]\n",
    "        \n",
    "        # retrieve a list of proteins by _id from DocDB\n",
    "        with MongoClient(self.db_uri) as client:\n",
    "            collection = client[self.db_name][self.collection_name]\n",
    "            cur = collection.find(\n",
    "                {'_id': {'$in': protein_ids}}, \n",
    "                projection={\"coords\": True, \"seq\": True}\n",
    "            )\n",
    "            return ((_convert_to_graph(protein), self.labels[protein['_id']]) \\\n",
    "                    for protein in cur)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.docs)\n",
    "        \n",
    "    def subset(self, indices):\n",
    "        '''Subset metadata docs inplace'''\n",
    "        self.docs = [self.docs[i] for i in indices]\n",
    "        return \n",
    "        \n",
    "def collate(samples):\n",
    "    graphs, targets = map(list, zip(*samples))\n",
    "    bg = dgl.batch(graphs)\n",
    "    return bg, torch.tensor(targets).unsqueeze(1).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa1d051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/pytorch/blob/7729581414962ac0a23ebd269f165f6a877490ae/torch/utils/data/dataset.py#L257-L312\n",
    "from typing import Iterator\n",
    "class BufferedShuffleDataset(data.IterableDataset):\n",
    "    r\"\"\"Dataset shuffled from the original dataset.\n",
    "    This class is useful to shuffle an existing instance of an IterableDataset.\n",
    "    The buffer with `buffer_size` is filled with the items from the dataset first. Then,\n",
    "    each item will be yielded from the buffer by reservoir sampling via iterator.\n",
    "    `buffer_size` is required to be larger than 0. For `buffer_size == 1`, the\n",
    "    dataset is not shuffled. In order to fully shuffle the whole dataset, `buffer_size`\n",
    "    is required to be greater than or equal to the size of dataset.\n",
    "    When it is used with :class:`~torch.utils.data.DataLoader`, each item in the\n",
    "    dataset will be yielded from the :class:`~torch.utils.data.DataLoader` iterator.\n",
    "    And, the method to set up a random seed is different based on :attr:`num_workers`.\n",
    "    For single-process mode (:attr:`num_workers == 0`), the random seed is required to\n",
    "    be set before the :class:`~torch.utils.data.DataLoader` in the main process.\n",
    "        >>> ds = BufferedShuffleDataset(dataset)\n",
    "        >>> random.seed(...)\n",
    "        >>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n",
    "    For multi-process mode (:attr:`num_workers > 0`), the random seed is set by a callable\n",
    "    function in each worker.\n",
    "        >>> ds = BufferedShuffleDataset(dataset)\n",
    "        >>> def init_fn(worker_id):\n",
    "        ...     random.seed(...)\n",
    "        >>> print(list(torch.utils.data.DataLoader(ds, ..., num_workers=n, worker_init_fn=init_fn)))\n",
    "    Arguments:\n",
    "        dataset (IterableDataset): The original IterableDataset.\n",
    "        buffer_size (int): The buffer size for shuffling.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset: data.IterableDataset\n",
    "    buffer_size: int\n",
    "    def __init__(self, dataset: data.IterableDataset, buffer_size: int) -> None:\n",
    "        super(BufferedShuffleDataset, self).__init__()\n",
    "        assert buffer_size > 0, \"buffer_size should be larger than 0\"\n",
    "        self.dataset = dataset\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def __iter__(self) -> Iterator:\n",
    "        buf = []\n",
    "        for x in self.dataset:\n",
    "            if len(buf) == self.buffer_size:\n",
    "                idx = random.randint(0, self.buffer_size - 1)\n",
    "                yield buf[idx]\n",
    "                buf[idx] = x\n",
    "            else:\n",
    "                buf.append(x)\n",
    "        random.shuffle(buf)\n",
    "        while buf:\n",
    "            yield buf.pop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c2f9aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "match = {\"is_AF\": {\"$exists\": True}}\n",
    "project = {\"y\": \"$is_AF\"}\n",
    "\n",
    "pipeline = [\n",
    "    {\"$match\": match},\n",
    "    {\"$project\": project},\n",
    "]\n",
    "\n",
    "dataset = ProteinDataset(\n",
    "    pipeline,\n",
    "    db_uri=uri,\n",
    "    db_name='proteins', \n",
    "    collection_name='proteins'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b18e171a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': ObjectId('611fed5aa9e1be4d05332068'), 'y': 1},\n",
       " {'_id': ObjectId('611fed5aa9e1be4d05332069'), 'y': 1},\n",
       " {'_id': ObjectId('611fed5aa9e1be4d0533206a'), 'y': 1},\n",
       " {'_id': ObjectId('611fed5ba9e1be4d0533206b'), 'y': 1},\n",
       " {'_id': ObjectId('611fed5ba9e1be4d0533206c'), 'y': 1}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3bcb6762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3151"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6327c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = BufferedShuffleDataset(dataset, buffer_size=128)\n",
    "random.seed(43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2728c776",
   "metadata": {},
   "source": [
    "## Speed test\n",
    "### single worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16432721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=9943, num_edges=19886,\n",
      "      ndata_schemes={'h': Scheme(shape=(21,), dtype=torch.float32)}\n",
      "      edata_schemes={}) tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "Epoch time: 0:00:13.633357\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "data_loader = data.DataLoader(\n",
    "    ds, \n",
    "    num_workers=0, \n",
    "    batch_size=32, \n",
    "    collate_fn=collate\n",
    ")\n",
    "# Epoch 0\n",
    "t0 = datetime.now()\n",
    "for bg, labels in data_loader:\n",
    "    if i == 0:\n",
    "        print(bg, labels)\n",
    "    i += 1\n",
    "    \n",
    "print('Epoch time:', datetime.now() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e53f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dgl.unbatch(bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c39c3bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=11052, num_edges=22104,\n",
      "      ndata_schemes={'h': Scheme(shape=(21,), dtype=torch.float32)}\n",
      "      edata_schemes={}) tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "# Epoch 1\n",
    "for bg, labels in data_loader:\n",
    "    print(bg, labels)    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5bedbfbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 1773, 0: 1378})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(dataset.labels.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c5c86e",
   "metadata": {},
   "source": [
    "### multiple workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "171613dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=527, num_edges=1054,\n",
      "      ndata_schemes={'h': Scheme(shape=(21,), dtype=torch.float32)}\n",
      "      edata_schemes={}) tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "Epoch time: 0:00:10.909427\n"
     ]
    }
   ],
   "source": [
    "ds = BufferedShuffleDataset(dataset, buffer_size=128)\n",
    "# def init_fn(worker_id):\n",
    "#     random.seed(42+worker_id)\n",
    "    \n",
    "i = 0\n",
    "data_loader = data.DataLoader(\n",
    "    ds, \n",
    "    num_workers=2, \n",
    "    batch_size=4, \n",
    "    collate_fn=collate,\n",
    ")\n",
    "# Epoch 0\n",
    "t0 = datetime.now()\n",
    "for bg, labels in data_loader:\n",
    "    if i == 0:\n",
    "        print(bg, labels)\n",
    "    i += 1    \n",
    "print('Epoch time:', datetime.now() - t0)\n",
    "# collection=None, Epoch time: 0:00:11.233891\n",
    "# collection=collection, Epoch time: 0:00:11.256877\n",
    "# with ... as client: Epoch time: 0:00:11.698559"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a9f1a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=986, num_edges=1972,\n",
      "      ndata_schemes={'h': Scheme(shape=(21,), dtype=torch.float32)}\n",
      "      edata_schemes={}) tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "# Epoch 1\n",
    "for bg, labels in data_loader:\n",
    "    print(bg, labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f68b70",
   "metadata": {},
   "source": [
    "## GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "647c34f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2520 631\n"
     ]
    }
   ],
   "source": [
    "# from dgl.dataloading import GraphDataLoader\n",
    "# from torch.utils.data.sampler import SubsetRandomSampler\n",
    "batch_size = 64\n",
    "train_dataset = ProteinDataset(\n",
    "    pipeline,\n",
    "    db_uri=uri,\n",
    "    db_name='proteins', \n",
    "    collection_name='proteins'    \n",
    ")\n",
    "\n",
    "num_examples = len(train_dataset.docs)\n",
    "num_train = int(num_examples * 0.8)\n",
    "\n",
    "idx = np.arange(num_examples)\n",
    "np.random.shuffle(idx)\n",
    "train_idx = idx[:num_train]\n",
    "test_idx = idx[num_train:]\n",
    "\n",
    "# split train/test\n",
    "train_dataset.subset(train_idx)\n",
    "test_dataset = ProteinDataset(\n",
    "    pipeline,\n",
    "    db_uri=uri,\n",
    "    db_name='proteins', \n",
    "    collection_name='proteins'    \n",
    ")\n",
    "test_dataset.subset(test_idx)\n",
    "\n",
    "print(len(train_dataset), len(test_dataset))\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    BufferedShuffleDataset(train_dataset, buffer_size=128),\n",
    "    batch_size=batch_size, \n",
    "    collate_fn=collate,\n",
    "    num_workers=8\n",
    ")\n",
    "\n",
    "test_dataloader = data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    collate_fn=collate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "385a594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from dgl.nn import GraphConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        g.ndata['h'] = h\n",
    "        return dgl.mean_nodes(g, 'h')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8ee9ab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with given dimensions\n",
    "dim_nfeats = len(d1_to_index)\n",
    "n_classes = 1\n",
    "model = GCN(dim_nfeats, 16, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b4c50486",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c1a5a023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "epoch: 1\n",
      "epoch: 2\n",
      "epoch: 3\n",
      "epoch: 4\n",
      "epoch: 5\n",
      "epoch: 6\n",
      "epoch: 7\n",
      "epoch: 8\n",
      "epoch: 9\n",
      "Time elapsed: 0:00:40.909794\n"
     ]
    }
   ],
   "source": [
    "t0 = datetime.now()\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    print('epoch:', epoch)\n",
    "    for batched_graph, labels in train_dataloader:\n",
    "        \n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        pred = model(batched_graph, batched_graph.ndata['h'])\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print('Time elapsed:', datetime.now() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6df12e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 26.49920760697306\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "num_tests = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batched_graph, labels in test_dataloader:\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        pred = model(batched_graph, batched_graph.ndata['h'].float())\n",
    "        num_correct += (pred.argmax(1) == labels).sum().item()\n",
    "        num_tests += len(labels)\n",
    "\n",
    "print('Test accuracy:', num_correct / num_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f909cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150fbcdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b84769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
