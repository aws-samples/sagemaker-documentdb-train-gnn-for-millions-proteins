{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "574dcd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ca9e2115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.12.0'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymongo\n",
    "pymongo.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b24511",
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets = json.load(open('DocumentDB_secrets.json', 'r')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01ba665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TLS enabled\n",
    "uri = 'mongodb://{}:{}@{}:27017/?tls=true&tlsCAFile=rds-combined-ca-bundle.pem&replicaSet=rs0&readPreference=secondaryPreferred&retryWrites=false'\\\n",
    "    .format(secrets['db_username'], secrets['db_password'], secrets['host'])\n",
    "\n",
    "client = MongoClient(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07b827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client['proteins']\n",
    "collection = db['proteins']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0457e091",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5f7be8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB.Polypeptide import d1_to_index, three_to_one\n",
    "\n",
    "# d1_to_index['X'] = len(d1_to_index) # encode uncommon residue as 20\n",
    "d1_to_index['X'] = 20\n",
    "\n",
    "def _convert_to_graph(protein):\n",
    "    '''\n",
    "    Convert a protein (dict) to a dgl graph\n",
    "    '''\n",
    "    coords = torch.tensor(protein['coords'])\n",
    "    X_ca = coords[:, 1]\n",
    "    # construct knn graph from C-alpha coordinates\n",
    "    g = dgl.knn_graph(X_ca, k=2)        \n",
    "    seq = protein['seq']\n",
    "    node_features = torch.tensor([d1_to_index[residue] for residue in seq])\n",
    "    node_features = F.one_hot(node_features, num_classes=len(d1_to_index)).to(dtype=torch.float)\n",
    "\n",
    "    # add node features\n",
    "    g.ndata[\"h\"] = node_features\n",
    "    return g    \n",
    "\n",
    "\n",
    "class ProteinDataset(data.IterableDataset):\n",
    "    \"\"\"\n",
    "    An iterable-style dataset for proteins in DocumentDB\n",
    "    Args:\n",
    "        - pipeline: an aggregation pipeline to retrieve data from DocumentDB\n",
    "    \"\"\"\n",
    "    def __init__(self, pipeline, db_uri='', db_name='', collection_name=''):\n",
    "        \n",
    "        self.db_uri = db_uri\n",
    "        self.db_name = db_name\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        with MongoClient(self.db_uri) as client:\n",
    "            collection = client[self.db_name][self.collection_name]\n",
    "            # pre-fetch the metadata as docs from DocumentDB\n",
    "            self.docs = [doc for doc in collection.aggregate(pipeline)]\n",
    "        # mapping document '_id' to label\n",
    "        self.labels = {doc['_id']: doc[\"y\"] for doc in self.docs}\n",
    "    \n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:  # single-process data loading, return the full iterator\n",
    "            protein_ids = [doc['_id'] for doc in self.docs]\n",
    "            \n",
    "        else:  # in a worker process\n",
    "            # split workload\n",
    "            start = 0\n",
    "            end = len(self.docs)\n",
    "            per_worker = int(math.ceil((end - start) / float(worker_info.num_workers)))\n",
    "            worker_id = worker_info.id\n",
    "            iter_start = start + worker_id * per_worker\n",
    "            iter_end = min(iter_start + per_worker, end)\n",
    "            \n",
    "            protein_ids = [doc['_id'] for doc in self.docs[iter_start:iter_end]]\n",
    "        \n",
    "        # retrieve a list of proteins by _id from DocDB\n",
    "        with MongoClient(self.db_uri) as client:\n",
    "            collection = client[self.db_name][self.collection_name]\n",
    "            cur = collection.find(\n",
    "                {'_id': {'$in': protein_ids}}, \n",
    "                projection={\"coords\": True, \"seq\": True}\n",
    "            )\n",
    "            return ((_convert_to_graph(protein), self.labels[protein['_id']]) \\\n",
    "                    for protein in cur)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.docs)\n",
    "        \n",
    "    def subset(self, indices):\n",
    "        '''Subset metadata docs inplace'''\n",
    "        self.docs = [self.docs[i] for i in indices]\n",
    "        return \n",
    "        \n",
    "def collate(samples):\n",
    "    graphs, targets = map(list, zip(*samples))\n",
    "    bg = dgl.batch(graphs)\n",
    "    return bg, torch.tensor(targets).unsqueeze(1).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1e964e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0,\n",
       " 'C': 1,\n",
       " 'D': 2,\n",
       " 'E': 3,\n",
       " 'F': 4,\n",
       " 'G': 5,\n",
       " 'H': 6,\n",
       " 'I': 7,\n",
       " 'K': 8,\n",
       " 'L': 9,\n",
       " 'M': 10,\n",
       " 'N': 11,\n",
       " 'P': 12,\n",
       " 'Q': 13,\n",
       " 'R': 14,\n",
       " 'S': 15,\n",
       " 'T': 16,\n",
       " 'V': 17,\n",
       " 'W': 18,\n",
       " 'Y': 19,\n",
       " 'X': 20}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db9c7369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/pytorch/blob/7729581414962ac0a23ebd269f165f6a877490ae/torch/utils/data/dataset.py#L257-L312\n",
    "from typing import Iterator\n",
    "class BufferedShuffleDataset(data.IterableDataset):\n",
    "    r\"\"\"Dataset shuffled from the original dataset.\n",
    "    This class is useful to shuffle an existing instance of an IterableDataset.\n",
    "    The buffer with `buffer_size` is filled with the items from the dataset first. Then,\n",
    "    each item will be yielded from the buffer by reservoir sampling via iterator.\n",
    "    `buffer_size` is required to be larger than 0. For `buffer_size == 1`, the\n",
    "    dataset is not shuffled. In order to fully shuffle the whole dataset, `buffer_size`\n",
    "    is required to be greater than or equal to the size of dataset.\n",
    "    When it is used with :class:`~torch.utils.data.DataLoader`, each item in the\n",
    "    dataset will be yielded from the :class:`~torch.utils.data.DataLoader` iterator.\n",
    "    And, the method to set up a random seed is different based on :attr:`num_workers`.\n",
    "    For single-process mode (:attr:`num_workers == 0`), the random seed is required to\n",
    "    be set before the :class:`~torch.utils.data.DataLoader` in the main process.\n",
    "        >>> ds = BufferedShuffleDataset(dataset)\n",
    "        >>> random.seed(...)\n",
    "        >>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n",
    "    For multi-process mode (:attr:`num_workers > 0`), the random seed is set by a callable\n",
    "    function in each worker.\n",
    "        >>> ds = BufferedShuffleDataset(dataset)\n",
    "        >>> def init_fn(worker_id):\n",
    "        ...     random.seed(...)\n",
    "        >>> print(list(torch.utils.data.DataLoader(ds, ..., num_workers=n, worker_init_fn=init_fn)))\n",
    "    Arguments:\n",
    "        dataset (IterableDataset): The original IterableDataset.\n",
    "        buffer_size (int): The buffer size for shuffling.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset: data.IterableDataset\n",
    "    buffer_size: int\n",
    "    def __init__(self, dataset: data.IterableDataset, buffer_size: int) -> None:\n",
    "        super(BufferedShuffleDataset, self).__init__()\n",
    "        assert buffer_size > 0, \"buffer_size should be larger than 0\"\n",
    "        self.dataset = dataset\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def __iter__(self) -> Iterator:\n",
    "        buf = []\n",
    "        for x in self.dataset:\n",
    "            if len(buf) == self.buffer_size:\n",
    "                idx = random.randint(0, self.buffer_size - 1)\n",
    "                yield buf[idx]\n",
    "                buf[idx] = x\n",
    "            else:\n",
    "                buf.append(x)\n",
    "        random.shuffle(buf)\n",
    "        while buf:\n",
    "            yield buf.pop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba6ae985",
   "metadata": {},
   "outputs": [],
   "source": [
    "match = {\"is_AF\": {\"$exists\": True}}\n",
    "project = {\"y\": \"$is_AF\"}\n",
    "\n",
    "pipeline = [\n",
    "    {\"$match\": match},\n",
    "    {\"$project\": project},\n",
    "]\n",
    "\n",
    "dataset = ProteinDataset(\n",
    "    pipeline,\n",
    "    db_uri=uri,\n",
    "    db_name='proteins', \n",
    "    collection_name='proteins'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae34df1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': ObjectId('611fed5aa9e1be4d05332068'), 'y': 1},\n",
       " {'_id': ObjectId('611fed5aa9e1be4d05332069'), 'y': 1},\n",
       " {'_id': ObjectId('611fed5aa9e1be4d0533206a'), 'y': 1},\n",
       " {'_id': ObjectId('611fed5ba9e1be4d0533206b'), 'y': 1},\n",
       " {'_id': ObjectId('611fed5ba9e1be4d0533206c'), 'y': 1}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d37bbb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3151"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "69d66afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = BufferedShuffleDataset(dataset, buffer_size=128)\n",
    "random.seed(43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42195cb",
   "metadata": {},
   "source": [
    "## Speed test\n",
    "### single worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "15b64790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=9943, num_edges=19886,\n",
      "      ndata_schemes={'h': Scheme(shape=(21,), dtype=torch.float32)}\n",
      "      edata_schemes={}) tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "Epoch time: 0:00:13.633357\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "data_loader = data.DataLoader(\n",
    "    ds, \n",
    "    num_workers=0, \n",
    "    batch_size=32, \n",
    "    collate_fn=collate\n",
    ")\n",
    "# Epoch 0\n",
    "t0 = datetime.now()\n",
    "for bg, labels in data_loader:\n",
    "    if i == 0:\n",
    "        print(bg, labels)\n",
    "    i += 1\n",
    "    \n",
    "print('Epoch time:', datetime.now() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "32587787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dgl.unbatch(bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "95583cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=11052, num_edges=22104,\n",
      "      ndata_schemes={'h': Scheme(shape=(21,), dtype=torch.float32)}\n",
      "      edata_schemes={}) tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "# Epoch 1\n",
    "for bg, labels in data_loader:\n",
    "    print(bg, labels)    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "26c3eb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 1773, 0: 1378})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(dataset.labels.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09694f7",
   "metadata": {},
   "source": [
    "### multiple workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0a5c46be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=527, num_edges=1054,\n",
      "      ndata_schemes={'h': Scheme(shape=(21,), dtype=torch.float32)}\n",
      "      edata_schemes={}) tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "Epoch time: 0:00:10.909427\n"
     ]
    }
   ],
   "source": [
    "ds = BufferedShuffleDataset(dataset, buffer_size=128)\n",
    "# def init_fn(worker_id):\n",
    "#     random.seed(42+worker_id)\n",
    "    \n",
    "i = 0\n",
    "data_loader = data.DataLoader(\n",
    "    ds, \n",
    "    num_workers=2, \n",
    "    batch_size=4, \n",
    "    collate_fn=collate,\n",
    ")\n",
    "# Epoch 0\n",
    "t0 = datetime.now()\n",
    "for bg, labels in data_loader:\n",
    "    if i == 0:\n",
    "        print(bg, labels)\n",
    "    i += 1    \n",
    "print('Epoch time:', datetime.now() - t0)\n",
    "# collection=None, Epoch time: 0:00:11.233891\n",
    "# collection=collection, Epoch time: 0:00:11.256877\n",
    "# with ... as client: Epoch time: 0:00:11.698559"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b575a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=986, num_edges=1972,\n",
      "      ndata_schemes={'h': Scheme(shape=(21,), dtype=torch.float32)}\n",
      "      edata_schemes={}) tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "# Epoch 1\n",
    "for bg, labels in data_loader:\n",
    "    print(bg, labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351f0dee",
   "metadata": {},
   "source": [
    "## GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d70f6484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 504 631\n"
     ]
    }
   ],
   "source": [
    "# from dgl.dataloading import GraphDataLoader\n",
    "# from torch.utils.data.sampler import SubsetRandomSampler\n",
    "batch_size = 64\n",
    "project = {\"y\": \"$is_AF\"}\n",
    "\n",
    "def match_by_split(split):\n",
    "    return {\"$and\": [\n",
    "        {\"is_AF\": {\"$exists\": True}},\n",
    "        {\"split\": split}\n",
    "    ]} \n",
    "\n",
    "train_dataset = ProteinDataset(\n",
    "    [\n",
    "        {\"$match\": match_by_split('train')},\n",
    "        {\"$project\": project},\n",
    "    ],\n",
    "    db_uri=uri,\n",
    "    db_name='proteins', \n",
    "    collection_name='proteins'    \n",
    ")\n",
    "\n",
    "\n",
    "valid_dataset = ProteinDataset(\n",
    "    [\n",
    "        {\"$match\": match_by_split('valid')},\n",
    "        {\"$project\": project},\n",
    "    ],\n",
    "    db_uri=uri,\n",
    "    db_name='proteins', \n",
    "    collection_name='proteins'    \n",
    ")\n",
    "\n",
    "test_dataset = ProteinDataset(\n",
    "    [\n",
    "        {\"$match\": match_by_split('test')},\n",
    "        {\"$project\": project},\n",
    "    ],\n",
    "    db_uri=uri,\n",
    "    db_name='proteins', \n",
    "    collection_name='proteins'    \n",
    ")\n",
    "\n",
    "print(len(train_dataset), len(valid_dataset), len(test_dataset))\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    BufferedShuffleDataset(train_dataset, buffer_size=128),\n",
    "    batch_size=batch_size, \n",
    "    collate_fn=collate,\n",
    "    num_workers=8\n",
    ")\n",
    "\n",
    "\n",
    "valid_dataloader = data.DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=batch_size, \n",
    "    collate_fn=collate,\n",
    ")\n",
    "test_dataloader = data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    collate_fn=collate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "70877514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from dgl.nn import GraphConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        g.ndata['h'] = h\n",
    "        return dgl.mean_nodes(g, 'h')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "32cedca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with given dimensions\n",
    "dim_nfeats = len(d1_to_index)\n",
    "n_classes = 1\n",
    "model = GCN(dim_nfeats, 16, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ecde24b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a126dceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "epoch: 1\n",
      "epoch: 2\n",
      "epoch: 3\n",
      "epoch: 4\n",
      "Time elapsed: 0:00:19.232999\n"
     ]
    }
   ],
   "source": [
    "t0 = datetime.now()\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    print('epoch:', epoch)\n",
    "    for batched_graph, labels in train_dataloader:\n",
    "        \n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        pred = model(batched_graph, batched_graph.ndata['h'])\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print('Time elapsed:', datetime.now() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0a7f78d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 27.20919175911252\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "num_tests = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batched_graph, labels in test_dataloader:\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        pred = model(batched_graph, batched_graph.ndata['h'].float())\n",
    "        num_correct += (pred.argmax(1) == labels).sum().item()\n",
    "        num_tests += len(labels)\n",
    "\n",
    "print('Test accuracy:', num_correct / num_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "deb2c931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "class EarlyStopper(object):\n",
    "    def __init__(self, patience, filename=None):\n",
    "        if filename is None:\n",
    "            # Name checkpoint based on time\n",
    "            dt = datetime.now()\n",
    "            filename = \"early_stop_{}_{:02d}-{:02d}-{:02d}.pth\".format(\n",
    "                dt.date(), dt.hour, dt.minute, dt.second\n",
    "            )\n",
    "            filename = os.path.join(\"/opt/ml/model\", filename)\n",
    "\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.filename = filename\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        \"\"\"Saves model when the metric on the validation set gets improved.\"\"\"\n",
    "        torch.save({\"model_state_dict\": model.state_dict()}, self.filename)\n",
    "\n",
    "    def load_checkpoint(self, model):\n",
    "        \"\"\"Load model saved with early stopping.\"\"\"\n",
    "        model.load_state_dict(torch.load(self.filename)[\"model_state_dict\"])\n",
    "\n",
    "    def step(self, score, model):\n",
    "        if (self.best_score is None) or (score > self.best_score):\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(\n",
    "                \"EarlyStopping counter: {:d} out of {:d}\".format(\n",
    "                    self.counter, self.patience\n",
    "                )\n",
    "            )\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        return self.early_stop\n",
    "\n",
    "\n",
    "class Meter(object):\n",
    "    \"\"\"Track and summarize model performance on a dataset for\n",
    "    (multi-label) binary classification.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.y_pred = []\n",
    "        self.y_true = []\n",
    "\n",
    "    def update(self, y_pred, y_true):\n",
    "        \"\"\"Update for the result of an iteration\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : float32 tensor\n",
    "            Predicted molecule labels with shape (B, T),\n",
    "            B for batch size and T for the number of tasks\n",
    "        y_true : float32 tensor\n",
    "            Ground truth molecule labels with shape (B, T)\n",
    "        \"\"\"\n",
    "        self.y_pred.append(y_pred.detach().cpu())\n",
    "        self.y_true.append(y_true.detach().cpu())\n",
    "\n",
    "    def roc_auc_score(self):\n",
    "        \"\"\"Compute roc-auc score for each task.\n",
    "        Returns\n",
    "        -------\n",
    "        list of float\n",
    "            roc-auc score for all tasks\n",
    "        \"\"\"\n",
    "        y_pred = torch.cat(self.y_pred, dim=0)\n",
    "        y_true = torch.cat(self.y_true, dim=0)\n",
    "        # This assumes binary case only\n",
    "        y_pred = torch.sigmoid(y_pred)\n",
    "        n_tasks = y_true.shape[1]\n",
    "        scores = []\n",
    "        for task in range(n_tasks):\n",
    "            task_y_true = y_true[:, task].numpy()\n",
    "            task_y_pred = y_pred[:, task].numpy()\n",
    "            scores.append(roc_auc_score(task_y_true, task_y_pred))\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "33224d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_a_train_epoch(args, epoch, model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    train_meter = Meter()\n",
    "    for batch_id, batch_data in enumerate(data_loader):\n",
    "        bg, labels = batch_data\n",
    "        bg = bg.to(args[\"device\"])\n",
    "        labels = labels.to(args[\"device\"])\n",
    "        logits = model(bg, bg.ndata[\"h\"])\n",
    "        # Mask non-existing labels\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\n",
    "            \"epoch {:d}/{:d}, batch {:d}/, loss {:.4f}\".format(\n",
    "                epoch + 1,\n",
    "                args[\"n_epochs\"],\n",
    "                batch_id + 1,\n",
    "                loss.item(),\n",
    "            )\n",
    "        )\n",
    "        train_meter.update(logits, labels)\n",
    "    train_score = np.mean(train_meter.roc_auc_score())\n",
    "    print(\n",
    "        \"epoch {:d}/{:d}, training roc-auc {:.4f}\".format(\n",
    "            epoch + 1, args[\"n_epochs\"], train_score\n",
    "        )\n",
    "    )\n",
    "\n",
    "def run_an_eval_epoch(args, model, data_loader):\n",
    "    model.eval()\n",
    "    eval_meter = Meter()\n",
    "    with torch.no_grad():\n",
    "        for batch_data in data_loader:\n",
    "            bg, labels = batch_data\n",
    "            bg = bg.to(args[\"device\"])\n",
    "            labels = labels.to(args[\"device\"])\n",
    "            logits = model(bg, bg.ndata[\"h\"])\n",
    "            eval_meter.update(logits, labels)\n",
    "    return np.mean(eval_meter.roc_auc_score())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8d617385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10, batch 1/, loss 0.5085\n",
      "epoch 1/10, batch 2/, loss 0.4835\n",
      "epoch 1/10, batch 3/, loss 0.4521\n",
      "epoch 1/10, batch 4/, loss 0.4260\n",
      "epoch 1/10, batch 5/, loss 0.4926\n",
      "epoch 1/10, batch 6/, loss 1.1380\n",
      "epoch 1/10, batch 7/, loss 1.1436\n",
      "epoch 1/10, batch 8/, loss 1.1436\n",
      "epoch 1/10, batch 9/, loss 0.3861\n",
      "epoch 1/10, batch 10/, loss 0.3728\n",
      "epoch 1/10, batch 11/, loss 0.3727\n",
      "epoch 1/10, batch 12/, loss 0.3709\n",
      "epoch 1/10, batch 13/, loss 0.7544\n",
      "epoch 1/10, batch 14/, loss 1.1461\n",
      "epoch 1/10, batch 15/, loss 1.1346\n",
      "epoch 1/10, batch 16/, loss 1.1240\n",
      "epoch 1/10, batch 17/, loss 0.4566\n",
      "epoch 1/10, batch 18/, loss 0.4170\n",
      "epoch 1/10, batch 19/, loss 0.4001\n",
      "epoch 1/10, batch 20/, loss 0.4013\n",
      "epoch 1/10, batch 21/, loss 0.7740\n",
      "epoch 1/10, batch 22/, loss 1.0579\n",
      "epoch 1/10, batch 23/, loss 1.0391\n",
      "epoch 1/10, batch 24/, loss 1.0340\n",
      "epoch 1/10, batch 25/, loss 0.4648\n",
      "epoch 1/10, batch 26/, loss 0.4487\n",
      "epoch 1/10, batch 27/, loss 0.4434\n",
      "epoch 1/10, batch 28/, loss 0.4387\n",
      "epoch 1/10, batch 29/, loss 0.7626\n",
      "epoch 1/10, batch 30/, loss 0.9913\n",
      "epoch 1/10, batch 31/, loss 0.9690\n",
      "epoch 1/10, batch 32/, loss 0.9559\n",
      "epoch 1/10, training roc-auc 0.5315\n"
     ]
    }
   ],
   "source": [
    "args = {'device': device, 'patience': 5, 'n_epochs': 10}\n",
    "\n",
    "stopper = EarlyStopper(args[\"patience\"])\n",
    "run_a_train_epoch(args, 0, model, train_dataloader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d96d7dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5/10, validation roc-auc 0.7721, best validation roc-auc 0.7721\n"
     ]
    }
   ],
   "source": [
    "# Validation and early stop\n",
    "val_score = run_an_eval_epoch(args, model, valid_dataloader)\n",
    "# early_stop = stopper.step(val_score, model)\n",
    "print(\n",
    "    \"epoch {:d}/{:d}, validation roc-auc {:.4f}, best validation roc-auc {:.4f}\".format(\n",
    "        epoch + 1, args[\"n_epochs\"], val_score, stopper.best_score\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88ac97d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
